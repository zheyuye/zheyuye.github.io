---
title: 'Literature Review of Query Reformulation'
date: 2021-01-28
permalink: /literature_review_of_query_reformulation
tags:
  - NLP
  - Survey

---

在多轮对话问答, 端到端对话聊天机器人等多个场景下, 不完整的句子在现有框架下的处理显得尤为棘手.  句子的不完整性可以体现在1. **指代词(coreference)**, 使得语句语焉不详指代不清, 没法在上下文缺失的情况下就单一句子理解含义 2. **省略词(ellipsi)**, 导致句子成份缺失, 上下文背景信息不完善. 如何解决这两个问题也引发了学界业界的广泛研究探讨, 从而衍生出两个子任务**Coreference Resolution**和**Information Completion**. 上述两个子任务可以统称为**Incomplete Utterance Rewriting (IUR**), 目的是将不完整的话语改写成语义等价但独立于语境的话语. 这篇文献综述选取了近年来比较有价值的**一些(23篇)**相关工作, 做了简要梳理, 包括相关公开数据集, 模型构造方法, etc.

## Contents

[toc]

##  [EMNLP 2019] Can You Unpack That? Learning to Rewrite Questions-in-Context (Elgohary et al., 2019)

关注于解决Question Answer任务中的Coreference和Ellipsi, 引入了基于上下文的问题改写任务 (**Task of question-in-context rewriting**), 文中又称de-contextualization

> We introduce the task of question-in-context rewriting: given the context of a conversation’s history, rewrite a context-dependent into a selfcontained question with the same answer.

从QuAC数据集中提取了40,527个questions, 构造了 **C**ontext **A**bstraction: **N**ecessary **A**dditional **R**ewritten **D**iscourse [(CANARD) 数据集]( [https://sites.google.com/view/qanta/projects/canard](https://sites.google.com/view/qanta/projects/canard)).

从下表中可以到人类对模型表现的对比(BLEU scores), 这仍然是一个难度较高且值得去探索的任务.

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210129003301053.png" alt="image-20210129003301053" style="zoom:70%;" />

## [COLING 2016] Non-sentential Question Resolution using Sequence to Sequence Learning (Kumar and Joshi, 2016)

这篇2016年的文章指出Question Answering (QA) system 当中non-sentential (incomplete) 的问题. 然而上下文缺失的情况下这些问题很难被问答系统很好的理解, 因此需要QA system借助历史对话数据来还原这些**non-sentential utterances (NSU)**, 从而可以更好地理解用户的意图.  

实验数据集无论在当时还是现在看来都是比较小的, 只包含了7220段对话, 其中每段对话包含先前的问题(Q1), 先前的答案 (A1), NSU question (Q2), and 改写后的句子 (R1), 如下图所示:

![image-20210201205938212](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210201205938212.png)

模型层面, 使用了RNN-based encoder-decoder 网络结构, 并分别设计了**Syntactic Sequence Model**和**Semantic Sequence Model**来学习语言和语义表征信息, 并通过一个集成模型选取上述两个模型的输出序列中与待改写问题**关键字重合度最高**的一个作为结果.

**Syntactic Sequence Model** : 

尽管训练语料很小, 但是字典大小的量级仍然维持在10k左右, 这在RNN encoder-decoder结构下需要大量的参数来维护一个输出向量, 这在算力有限的2016年仍然是一个挑战. 常见的处理方式是, 维护一个较小的字典大小, 然后把未知词都标识为UNK. 

Syntactic Sequence Model将语料库中OOV的单词替换为带有数字编号的UNK, 这个数字编号是它在对话中的相对位(如下图). symbols(UNK1, UNK2, UNK3, UNK4)在不同对话当中是共享的, 这使得该模型能够在不同的对话中学习语言结构信息.  

![image-20210202163359796](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210202163359796.png)

**Semantic Sequence Model:** 

Syntactic Sequence Model只关注OOV词在序列中的位置，分配一个新的未知符号UNK，完全抛弃了OOV词之间的语义相似性.  下面这个两个例子(a) (b)中的UNK拥有完全相同的位置, 但是所期望得到的R1其实是大相径庭的. 

![image-20210202163434481](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210202163434481.png)

对此, Semantic Sequence Model的处理方式是, 将UNK词通过word2Vec的结果进行k-means聚类, 生成Category Label(CL). 上述(a)(b)两个例子中的UNK的类别分类如下

![image-20210202165217450](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210202165217450.png)

## [SIGIR 2017] Incomplete Follow-up Question Resolution using Retrieval based Sequence to Sequence Learning (Kumar and Joshi, 2017)

上文的两位作者, 在SIGIR 2017也有后续的工作展现. 好像很重要, 找不到Access.

https://openreview.net/forum?id=Sy41ErWd-B

## [ACL 2016] Incorporating Copying Mechanism in Sequence-to-Sequence Learning (Gu et al., 2016)

作者来自香港大学和华为诺亚方舟实验室. 这篇文章参考了人类在对话当中喜欢重复复述名词实体和较长短语的复读机行为, 提出了基于sequence-to-sequence的COPYNET​, 很好得将传统的生成模式和拷贝模式集成到一起. 

在NLP中, sequence-to-sequence这样的经典模型试图将以原始词汇表组成的原始序列映射到一个目标词汇表组成的目标序列中. 原始词汇表和目标词汇表可以是相同的, 或者有很大一部分是重叠的, 就像文本摘要任务; 也可以是完全不同的, 比如机器翻译场景下. 还有许多这样的任务都要求模型能够在目标序列中产生出现在源序列中, 但相对于目标词汇表而言out-of- vocabulary(OOV)的token. 一个简单而直观的想法就是直接从原始序列复制token, 来解决这个oov问题.

**“copy mechanism”**被拟用来模拟人类沟通交流过程中“复述”的行为, 是一个选择原始序列中的某个片段，然后将该片段拷贝到目标序列中的过程. 

**模型构造: **

![image-20210129011938501](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210129011938501.png)

2021年了, sequence-to-sequence, RNN,  Attention Mechanism这些都老生常谈了, 不再赘述了. 

CopyNet的工作方式是在每个decoding step中计算目标词汇表中的每个token和源序列中的每个token的概率。通过这种方式，CopyNet获得一个目标词汇表以及原始序列构成的**extended vocabulary**，允许模型潜在地生成相对目标词汇表OOV的token.  在CopyNet中, Encoder采用了一个Bidi-RNN模型, 输出一个隐藏层$M$作为short-term memory; Decoder基于Canonical RNN-decoder改造, 主要有以下三点不同:

* Prediction: 存在生成模式和拷贝模式两种不同的词汇解码模式，CopyNet的预测基于这两个模式的混合概率模型
* State Update: 更新t-th step的状态时，COPYNET不仅仅使用用t-1-th step预测结果的词向量，而且使用M中特定位置的hidden state
* Reading M: Copynet选择性地读取M的值, 获取内容信息以及位置信息有效混合信息

拓展词汇表extended vocabulary可以定义为:

$$
\text{EV} = V \cup X \cup \text{[UNK]}
$$
其中目标词汇表$V = \{v_₁, \cdots , v_ₙ\}$ 原始序列$X = \{x_₁, \cdots , x_ₙ\}$ , 未知词 [UNK]. 该模型将能够在拓展词汇表中自由地选择词汇。具体而言, 在解码步骤的t-th step, token $y_t$的概率分布为:

$$
p\left(y_{t} \mid \cdot\right)=\underbrace{p_{g}\left(y_{t} \mid \cdot\right)}_{\text {generation prob. }}+\overbrace{p_{c}\left(y_{t} \mid \cdot\right)}^{\text {copy prob. }}
$$


![image-20210129014922886](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210129014922886.png)

根据上图的四种情况, 分别计算generation probability $p_{g}\left(y_{t} \mid \cdot\right)$ 和copy probability  $p_{c}\left(y_{t} \mid \cdot\right)$:

$$
p_{g}\left(y_{t} \mid \cdot\right)=\left\{\begin{array}{ll}\frac{1}{Z} e^{\psi_{g}\left(y_{t}\right)} & \text { if } y_{t} \in V \\ 0 & \text { if } y_{t} \in X \text { and } y_{t} \notin V \\ \frac{1}{Z} e^{\psi_{g}(U N K)} & \text { if } y_{t} \notin X \cup V
\end{array}\right.
$$

$$
p_{c}\left(y_{t} \mid \cdot\right)=\left\{\begin{array}{ll}\frac{1}{Z} \sum_{j: x_{j}=y_{t}} e^{\psi_{c}\left(x_{j}\right)} & \text { if } y_{t} \in X \\ 0 & \text { otherwise }\end{array}\right.
$$

其中$Z=\sum_{v \in V \cup\{\mathrm{UNK}\}} e^{\psi_{g}(v)}+\sum_{j=1}^{m} e^{\psi_{c}\left(x_{j}\right)}$是一个normalization term, $ \psi_{g}(\cdot)$是generation的得分函数,  $\psi_{c}(\cdot)$是copy的得分函数. 

**状态更新**

t-1-th step时刻, $y_{t-1}$的hidden state表示为$\left[\mathbf{e}\left(y_{t-1}\right) ; \zeta\left(y_{t-1}\right)\right]^{\top}$, 由$y_{t-1}$的词向量与状态权重和拼接而成, 理解为**attentive read** + **selective read**, weighted sum 函数$\zeta(\cdot)$定义为:

$$
\begin{array}{l}\zeta\left(y_{t-1}\right)=\sum_{\tau=1}^{T_{S}} \rho_{t \tau} \mathbf{h}_{\tau} \\ \rho_{t \tau}=\left\{\begin{array}{ll}\frac{1}{K} p\left(x_{\tau}, \mathbf{c} \mid \mathbf{s}_{t-1}, \mathbf{M}\right), & x_{\tau}=y_{t-1} \\ 0 & \text { otherwise }\end{array}\right.\end{array}
$$

**M的混合解析**

一个词的语义和它在X中的位置都将被一个经过适当训练的编码器RNN编码到M中的隐藏状态中。

在生成模式下, attentive read占据主导, 其主要由受到语义信息和语言模型来驱动，因此阅读M上的信息时位置更加跳跃自由; 在拷贝模式下，对M的selective read往往受到位置信息的引导, 从而采取**非跳跃式僵化移动(rigid move)**的做法, 往往涵盖连续的多个词，包括未知词. 

位置信息的更新方式如下:

$$
\zeta\left(y_{t-1}\right) \stackrel{\text { update }}{\longrightarrow} \mathbf{s}_{t} \stackrel{\text { predict }}{\longrightarrow} y_{t} \stackrel{\text { sel. read }}{\longrightarrow} \zeta\left(y_{t}\right)
$$

## [ACL 2017] Get To The Point: Summarization with Pointer-Generator Networks (See  et  al.,  2017)

论文原作者Abigail See的解读: [http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html](http://www.abigailsee.com/2017/04/16/taming-rnns-for-better-summarization.html)

**Two Problems:**

基于RNN + attention的abstractive summarization方法面临两个问题

1. 没法准确地再现事实细节(**复制某个词**), 特别面对oov或者不常见的词 
   * rare words的word embedding不够强
   * 一些名词性质的单词尽管拥有较强的representation, 但是其embedding倾向于聚集在一起，这可能会在试图还原单词时造成混淆

2. 经常自我重复
   * decoder过度依赖于其输入, 即先前生成的词, 而不是在decoder中存储长程信息

**How to Fix:**

**Easier Copying with Pointer-Generator Networks:** 为了解决**Problem 1** (inaccurate copying), 提出了*pointer-generator network*. 这是一个混合网络，可以选择通过point从原本复制单词，同时保留从 fixed vocabulary生成单词的能力. 具体模型构造可以概括为计算attention distribution $a$, vocabulary distribution $P_\text{vocab}(w)$, generation probability $p_{\text{gen}} \in (0,1)$

$$
P_\text{final}(w) = p_{\text{gen}} P_\text{vocab}(w) + (1 - p_{\text{gen}})\sum_{i:w}^{i=w} a_i
$$

生成单词w的概率 = 从词汇表生成的概率 + 复制原文某一个处文本的概率

**Eliminating Repetition with Coverage**: 为了解决**Problem 2**(repetitive summaries), 提出*coverage*方法使用attention distribution记录覆盖率, 为重复部分添加惩罚机制

$$
c_t =  \sum^{t-1}_{t\prime=0} a^{t\prime}
$$


即, 一个原始单词的覆盖率等于它迄今所接收到的attention scores的和

## [ACL 2019] Improving Multi-turn Dialogue Modelling with Utterance ReWriter (Su  et  al.,  2019)

这篇来自微信人工智能模式识别中心的文章, 利用指针网络引入了一种基于transformer的重写体系结构, 同时另外一大贡献是收集了一个带有人工注释的新数据集.

**数据集构造**

作者从几个主流的中文社交媒体平台上抓取了200k个候选的多轮会话数据, 从中整理出了40k正负比例平衡的高质量语料，其中正例指包含省略或指代, 负例中的句子则拥有完整的语义表达, 不需要重写. 

下面这张表是其中随机2000个对话数据中, 省略or指代出现频率的分析结果.  只有不到30%的utterances既没有指代也没有省略，相当多的utterances二者都有, 这进一步证实了在多轮对话中处理指代和省略的重要性.

![image-20210128234324479](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210128234324479.png)

**模型构造**

使用了基于Transformer encoder-decoder结构的copy network, 试图根据历史对话数据$H$以及需要被改写的最后一轮utterances $U_n$ 学到一个mapping function $p(R \mid (H, U_n ))$ 从而得到改写后的句子$R$. 跟经典的Transformer类似, 对于每一个token $w_i$ , 它的词向量由word embedding 以及positional embedding相加得到, 再次基础上本文引入了第三种embedding ——turn embedding 用以区分不同轮数:

$$
I\left(w_{i}\right)=W E\left(w_{i}\right)+P E\left(w_{i}\right)+T E\left(w_{i}\right)
$$

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210129011251221.png" alt="image-20210129011251221" style="zoom: 80%;" />

上图显示了模型的整体架构, Encoder和Decoder的部分几乎就是typical的Transformer, 这边不再赘述了. 在最后decoder的输出层, 我们希望模型能在decoding阶段的每一步都能学习到是否copy来自历史对话数据H的字词. 这边一个额外的probability λ 需要在每一步被计算来决定是否从context 或者原始待改写句子 $U_n$中复制字词:

$$
\lambda=\sigma\left(\boldsymbol{w}_{d}^{\top} \mathbf{D}_{t}^{L}+\boldsymbol{w}_{H}^{\top} \mathbf{C}(H)_{t}^{L}+\boldsymbol{w}_{U}^{\top} \mathbf{C}\left(U_{n}\right)_{t}^{L}\right)
$$

* λ = 1, If $U_n$ contains neither coreference nor information omission
* λ = 0, when a coreference or omission is detected

通过maximizing $p(R \mid (H, U_n ))$ 来训练这个端到端模型:

$$
p\left(R_{t}=w \mid H, U_{n}, R_{<t}\right)=\lambda \sum_{i:\left(w_{i}=w\right) \wedge\left(w_{i} \in \mathrm{H}\right)} a_{t, i} + (1-\lambda) \sum_{j:\left(w_{j}=w\right) \wedge\left(w_{j} \in U_{n}\right)} a_{t, j}^{\prime} \\ a=\operatorname{Attention}\left(\mathbf{M}^{(L)}, \mathbf{E}_{U_{n}}^{(L)}\right) \\ a^{\prime}=\operatorname{Attention}\left(\mathbf{M}^{(L)}, \mathbf{E}_{H}^{(L)}\right)
$$

$a$ and $a\prime$ 分别是H和$U_n$的attention scores. 此时注意机制负责从对话历史H或者$U_n$中找到适当的共指信息或缺省信息

**结果分析: **

以下四种不同类型的模型比较中, T-Ptr-Gen生成质量不高, 比单独的Copy network(2)效果更差

1. (L/T)-Gen: Pure generation-based model. Words are generated from **a fixed vocabulary.** (worst)
2. (L/T)-Ptr-Net: Pure **pointer-based** model. Words can only becopied from the input
3. (L/T)-Ptr-Gen: Hybrid **pointer+generation** model. Words can be either copied from the input or generatedfrom a fixed vocabulary.
4. (L/T)-Ptr-λ: Our proposed model which split the attention by a coefficient λ.  (best)

## [EMNLP 2019] Improving Open-Domain Dialogue Systems via Multi-Turn Incomplete Utterance Restoration (Pan et  al.,  2019)

来自腾讯AI Lab的工作, 文章开源了一个大型多轮对话数据集[Restoration-200K](https://ai.tencent.com/ailab/nlp/dialogue/#datasets), 提出了**"pick-and-combine"**的方法试图从对话系统中的上下文内容中还原不完整的语句, 并对比其和Syntactic(Kumar and Joshi, 2016), Sequence-to-Sequence model (Seq2Seq)和Pointer Generative Network (See  et  al.,  2017)的模型表现.

**数据集构造: **

原始语料从豆瓣小组爬取, 数据样本大小为200k, 其中每个样本中包含6条语句. 

辛苦的五人标注团队花了六个月时间完成了以下两个标注任务:

1. utterance是否省略先前话语中所产生的概念或实体, 即是否需要改写(与上文的正负样本相同)
2. 对于正例, 人工重写不完整的语句. 重写后的语句被要求尽可能使用原文使用过的字词, 来减少改写句子的多样性. 再迫不得已的情况下, 可以使用预先设定好的一个**规模较小的常用单词列表**来保证句子的流畅性. 后续校验过程发现只有4.8%的句子不满足这个要求, 且都被剔除于数据集之外了.

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210201200228749.png" alt="Statistics of Restoration-200K" style="zoom:80%;" />

从上表的数据集统计信息中, 我们可以注意, 这200k大小的数据集中有60%的句子是不完整的. 相比于Kumar and Joshi, 2016 (Avg. utterance lenght=3.52), Restoration-200K的平均语句长度更长, 理论上单句话包含更多的信息.

**Pick-and-Combine(PAC) Model: **

作者表示, 还原后的utterances与Original  utterances的重叠率是100%, 而与Previous utterances的重叠率只有17.7%. 因而模型在训练过程中, 会倾向于简单地复制原始utterances而没法有效地改写该语句. PAC Model将改写分为了两个步骤

1. **Pick process**: 使用BERT, 识别**Previous utterances**中被省略的单词 (序列标注任务).
2. **Combine stage**:  根据已识别的省略词恢复原话语, 实验验证下BERT效果不佳. 将selected omitted words附加到Original utterances的后方, 然后将这两个序列输送到一个pointer generative network中.  (标注过程中使用到的额外字典在这边应该也需要引入进来, 确保PGN能够尽可能地还原restored utterances)

![image-20210202155834750](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210202155834750.png)

机器指标和人工指标下, PAC都表现出了卓越的性能表现.

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210202170651315.png" alt="image-20210202170651315" style="zoom: 67%;" />

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210202171237588.png" alt="image-20210202171237588" style="zoom:67%;" />

## [AAAI 2019] FANDA: A Novel Approach to Perform Follow-Up Query Analysis (Liu  et  al.,  2019a)

Slides: https://siviltaram.github.io/files/fanda-slides.pdf

在Natural Language Interfaces to Databases (NLIDB)场景中, 用户可以使用自然语言搜索数据库，而不是使用类似sql的查询语言. 然而在用户多轮查询的过程中, 经常出现极简问, 后续查询(Follow-up query)等等例子, 这就需要充分解析上下文信息从而理解用户查询的真实意图. **Follow-up query**可以定义为上下文无关问题(precedent query)的后续问题, 前者我们可以称之为**precedent query**, 重写后融合上下文语义信息的query则被称为fused query.

从[WikiSQL](https://arxiv.org/abs/1709.00103)中筛选了一个数据集[FollowUp](https://github.com/SivilTaram/FollowUp), 其中包含了1000个问题的三元组跨越120个不同的表, 以下是一些数据集中的样本示例. 另外一大贡献则是提出**F**ollow-up **AN**alysis for **DA**tabases (FANDA)方法

![image-20210202172908053](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210202172908053.png)

precedent query($x$)与follow-up query($y$)往往会有大量冲突的语义结构. FANDA特此将symbol-level和segment-level两种结构的信息都纳入考量, 前者代表单词, 后者代表与SQL语句相关的短语. 具体而言, FANDA的结构可以细化为下图所示的三个模块:

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210202190009315.png" alt="image-20210202190009315" style="zoom:80%;" />

**Anonymization**

这一步骤中, query当中的analysis-specific words会被挖掘出来.  Analysis-specific words是SQL语句的具体参数, 可以分为下图八种. 可以在FANDA的结构图中看到, 这八种analysis-specific words会被替换为对应的Symbol. 而剩下是组成句式的修饰词(rhetorical words).

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210202190638073.png" alt="image-20210202190638073" style="zoom:67%;" />

**Generation**

这种挖掘字词语义却忽略了上下文信息的行为, 跟Kumar and Joshi (2016)中的Semantic Sequence Model有点像. 为了弥补这种上下文语义信息的缺失, segment structure被用来捕捉这些修饰词, 通常基于SQL参数和简单常识 (Table 3). 在Generation中, Symbols被组合产生所有可能的segment sequences (考虑到ellipsis的存在, 图一有12种可能), 然后通过一个ranking model来选择得分最高的segment sequence. 

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210202194231116.png" alt="image-20210202194231116" style="zoom:67%;" />

**Fusion**

Fusion模块可拆分为两步走战略:

1. 第一步: 找到x与y之间, sql成份冲突的部分(相同或不兼容的语义). 

2. 第二步: 我们用一个segment替换另一个segment来融合两个segments, 以解决上述冲突.

第二步中的"替换"操作可以具体为Refine 和 Append:

* Refine: “How much money has Smith earned? How about Bill Collins?”
* Append:  “How much money has Smith earned? Compare with Bill Collins.”

**Ranking model**

区分两种Intents: **Refine & Append**, 并参照NER的方法, 对Seg进行IOB标注, BiDi-LSTM+CRF进行ranking , 取得分最高的表达作为最后意图. 

对于sql语句的标注, 这是一件毕竟费时费力的事情, 本文使用了少量fused query的ground truth进行弱监督学习. 

## [EMNLP 2019] A Split-and-Recombine Approach for Follow-up Query Analysis (Liu  et  al.,  2019b)

Slides: https://siviltaram.github.io/files/split-slides.pdf

code: https://github.com/microsoft/EMNLP2019-Split-And-Recombine

来自与上文工作的同一批作者, inspired by 自己之后又把自己超了👍. 提出了**S**pli**T**-And-**R**ecombine (STAR)模型, 正如其名字所暗示的, STAR先将precedent query($x$)与follow-up query($y$)拆分为多个span, 然后再重新组合到一起生成restated query(又换了一个名字), 如下图所示:

![image-20210202202549973](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210202202549973.png)

![image-20210202203225934](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210202203225934.png)

要maximize的目标为:

$$
P_{\text {model }}(\mathbf{z} \mid \mathbf{x}, \mathbf{y})=\sum_{q \in \mathcal{Q}} P_{\text {split }}(q \mid \mathbf{x}, \mathbf{y}) P_{\text {rec }}(\mathbf{z} \mid q)
$$

其中$\mathcal{Q}$为split(x, y)的所有可能. 由于缺少用于splitting和recombination的注释，很难直接执行监督学习, 再这边强化学习大法又来了:

$$
\mathcal{L}_{\mathrm{rl}}=\mathbb{E}\left[\sum_{\tilde{\mathbf{z}} \in \mathcal{Z}} \sum_{q \in \mathcal{Q}} P_{\mathrm{split}}(q \mid \mathbf{x}, \mathbf{y}) P_{\operatorname{rec}}(\tilde{\mathbf{z}} \mid q) r(\mathbf{z}, \tilde{\mathbf{z}})\right]
$$

其中$\mathcal{Z}*$ 是所有restated  query的候选空间, $r$代表由比较$\mathbf{z}$和标注$z$产生的reward.

**Phase I: Split**

把span分割任务再再再当成序列标注任务(Split or Retain). 使用BiDAF (Seo et al., 2017)获取$x,y$之间的语义交互. Embedding由character, word and sentence拼接而来: $\phi=\left[\phi_{c} ; \phi_{w} ; \phi_{s}\right]$

**Phase II: Recombine**

直接我们只需将precedent query中的span替换为follow query中冲突的span，以生成restated query.



我们直接略过强化学习的部分直接看结果:

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210202201847378.png" alt="image-20210202201847378" style="zoom:80%;" />

## [NAACL 2018 Short] Higher-order coreference resolution with coarse-tofine inference (Lee et  al.,  2018)

Code: https://github.com/kentonl/e2e-coref/

解决高阶指代消解问题, 主要是globally inconsistent的问题:

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210202211428782.png" alt="image-20210202211428782" style="zoom:80%;" />

**Baseline:** Bidirectional LSTMs
$$
s(i, j)=s_{\mathrm{m}}(i)+s_{\mathrm{m}}(j)+s_{\mathrm{a}}(i, j) \\ 
s_{\mathrm{m}}(i) =\boldsymbol{w}_{\mathrm{m}}^{\top} \mathrm{FFNN}_{\mathrm{m}}\left(\boldsymbol{g}_{i}\right) \\
s_{\mathrm{a}}(i, j) =\boldsymbol{w}_{\mathrm{a}}^{\top} \mathrm{FFNN}_{\mathrm{a}}\left(\left[\boldsymbol{g}_{i}, \boldsymbol{g}_{j}, \boldsymbol{g}_{i} \circ \boldsymbol{g}_{j}, \phi(i, j)\right]\right)
$$

最后得到antecedent distribution  $P\left(y_{i}\right)$

$$
P\left(y_{i}\right)=\frac{e^{s\left(i, y_{i}\right)}}{\sum_{y^{\prime} \in \mathcal{Y}(i)} e^{s\left(i, y^{\prime}\right)}}
$$

**Higher-order approach**

高维信息其实体现在多轮对话中, 简单地使用LSTM连接两个mention会导致长程信息的消失. 这篇文章提出了一个推理方法, 充分考虑前文的信息. 具体而言是多轮的迭代(iterations), 假设N轮, 通过attention机制将前一轮的表征信息$g_j^{n-1}$加入进来, 来计算i,j两个位置互相是mention的概率:

$$
P_{n}\left(y_{i}\right)=\frac{e^{s\left(\boldsymbol{g}_{i}^{n}, \boldsymbol{g}_{y_{i}}^{n}\right)}}{\sum_{y \in \mathcal{Y}(i)} e^{\left.s\left(\boldsymbol{g}_{i}^{n}, \boldsymbol{g}_{y}^{n}\right)\right)}}
$$

其中$s$还是baseline当中用到的得分方程. 每一轮的span representation $g^n_i$的更新方式如下:

$$
\begin{aligned}
\boldsymbol{f}_{i}^{n} &=\sigma\left(\mathbf{W}_{\mathrm{f}}\left[\boldsymbol{g}_{i}^{n}, \boldsymbol{a}_{i}^{n}\right]\right) \\
\boldsymbol{g}_{i}^{n+1} &=\boldsymbol{f}_{i}^{n} \circ \boldsymbol{g}_{i}^{n}+\left(\mathbf{1}-\boldsymbol{f}_{i}^{n}\right) \circ \boldsymbol{a}_{i}^{n}
\end{aligned}
$$

其中$a_{i}^{n}=\sum_{y_{i} \in \mathcal{Y}(i)} P_{n}\left(y_{i}\right) \cdot g_{y_{i}}^{n}$

**Coarse-to-fine antecedent pruning**
上述推理过程比较耗时, 需要加入一些剪枝技巧. 引入一个alternate bilinear的得分函数$s_{\mathrm{c}}(i, j)=g_{i}^{\top} \mathbf{W}_{\mathrm{c}} g_{j}$, $s_c(i,j)$相较于 $s_a(i,j)$计算量要小得多. 这样我们可以将 $s(i,j)$改写为
:
$$
s(i, j)=s_{\mathrm{m}}(i)+s_{\mathrm{m}}(j)+s_{\mathrm{c}}(i, j)+s_{\mathrm{a}}(i, j)
$$

通过三个步骤, 分别得到

1. $s_{\mathrm{m}}(i)$的top M
2. $s_{\mathrm{m}}(i)+s_{\mathrm{m}}(j)+s_{\mathrm{c}}(i, j)$的top K
3. $s(i, j)$的top 1

## [EMNLP 2019 Short] BERT for Coreference Resolution: Baselines and Analysis (Joshi  et  al.,  2019)

一篇将BERT应用到Coreference Resolution的工作, 恰如其名. 分别将上文中c2f-c2fcore (Lee et  al.,  2018)中的LSTM替换为BERT, Elmo对比了模型表现. e2e-coref为不使用分布式语义的模型版本.

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210202214753196.png" alt="image-20210202214753196" style="zoom:80%;" />

因为max segment len的存在, 对于超过max segment len的文档, 采用以下两种变体作为处理方案:

**Independent:** 超过max segment len就截断

**Overlap:** 超过max segment len的文档, 通过滑动窗口的形式生成多个片段. 再通过$f$拼接起来:

$$
\begin{array}{l}
\mathrm{f}=\sigma\left(\mathrm{w}^{T}\left[\mathrm{r}_{1} ; \mathrm{r}_{2}\right]\right) \\
\mathrm{r}=\mathrm{f} \cdot \mathrm{r}_{1}+(1-\mathrm{f}) \cdot \mathrm{r}_{2}
\end{array}
$$

## [EMNLP 2020] Incomplete Utterance Rewriting as Semantic Segmentation (Liu  et  al.,  2020)

将incomplete utterance rewriting转化为一个语义分割任务(Semantic Segmentation), 构造一个word-level edit matrix $Y = M \times N$ 包含三种不同的编辑类型: None, Substitute and Insert. 提出了Rewritten U-shaped Network (RUN)来构造这个$Y$, 并进行随后的字词替换插入操作.

![image-20210129011502979](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210129011502979.png)

正如上图模型架构所描绘的, 模型(RUN)主要由以下三个组件构成:

* **Context layer:** GloVe  + BiLSTM (c and x are jointly encoded)
* **Encoding layer (concentrate on local rather than global information):**  concatenating 1. element-wise similarity (Ele Sim.) 2. cosine similarity (Cos Sim.) and  3. learned bi-linear similarity (Bi-Linear Sim.) -> D-dimensional feature vector $ F \in \R^{M \times N \times D}$
* **Segmentation layer (To capture global information):**  Conv + pool + skip connect + ffn ->  $ Y \in \R^{M \times N}$

上述架构同样可以使用基于预训练模型(e.g. BERT, etc)获取分布式表征信息, RUN + BERT表现出相较RUN更有说服力的实验结果

生成前需要进行一步standardization确保所有的$Y$都是长方形, based on Hoshen–Kopelman, 并添加**Connection Words**保证句子流畅度

![image-20210128191115163](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210128191115163.png)

在REWRITER数据集 (Su  et  al.,  2019)上的结果:

![image-20210204144752188](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210204144752188.png)

## [EMNLP 2019] Unsupervised Context Rewriting for Open Domain Conversation (Zhou et  al.,  2019)

提出了**Context rewriting network (CRN)**, CRN基于 Bidi-GRU + attention的encoder-decoder模型架构实现, 在decoding阶段使用CopNet架构从原文中复制文本. 在没有真实标注的条件下, CRN model 先在Pseudo Data进行预训练, 在生成阶段也**关心Response与改写语句的交互关系**, 进而提高回复的准确率.

**模型构造 **

在Decoder当中, 每一个step $t$接受融合后的多元信息context $c$, last utterance $q$以及上一步的hidden state $s_t$作为输入, 从而权衡是否需要直接从context中拷贝字词到pseudo rewritten candidate $q^*$

$$
z_{t}=W_{f}^{T}\left[s_{t} ; \sum_{i=1}^{n q} \alpha_{q_{i}} h_{q_{i}} ; \sum_{i=1}^{n c} \alpha_{c_{i}} h_{c_{i}}\right]+b
$$

其中$\alpha_q$, $\alpha_c$都是注意力权重, 根据以下公式计算可得:

$$
\alpha_{i}=\frac{\exp \left(e_{i}\right)}{\sum_{j=1}^{n} \exp \left(e_{j}\right)} \\
e_{i}=h_{i} W_{a} s_{t}
$$

最后可以用copy mechanism来预测下一个目标词汇, 根据这个条件概率公式:

$$
p\left(y_{t} \mid s_{t}, H_{Q}, H_{C}\right) =p_{p r}\left(y_{t} \mid z_{t}\right) \cdot p_{m}\left(\operatorname{pr} \mid z_{t}\right) + p_{c o}\left(y_{t} \mid z_{t}\right) \cdot p_{m}\left(c o \mid z_{t}\right) \\
L_{M L E}=-\frac{1}{N} \sum_{i=1}^{n} \log \left(p\left(y_{t} \mid s_{t}, H_{Q}, H_{C}\right)\right)
$$

$y_t$**是回复中的t-th词**,  $p_{pr}(y_t \mid z_t)$和$p_{co}(y_t \mid z_t)$分别是predict-mode和copy-mode的候选词概率分布, $\phi_{pr}(\cdot)$ 和$\phi_{co}(\cdot)$分别是其得分函数. 并且由一个额外的$p_m(\cdot \mid \cdot)$来控制选择这两个模式的概率

$$
p_{m}\left(p r \mid z_{t}\right)=\frac{e^{\psi_{p r}\left(y_{t}, H_{Q}, H_{C}\right)}}{e^{\psi_{p r}\left(y_{t}, H_{Q}, H_{C}\right)}+e^{\psi_{c o}\left(y_{t}, H_{Q}, H_{C}\right)}}
$$

**Pre-training with Pseudo Data:**

因为数据集中并没有改写句的标注，因此作者从对话历史中抽取关键词来构造模拟数据. 

**Key Words Extraction:** 为了寻找共同信息量大的words, 作者使用了**pointwise mutual information (PMI)**来抽取文本中的关键词

$$
\operatorname{PMI}\left(w_{c}, w_{r}\right)=-\log \frac{p_{c}\left(w_{c}\right)}{p\left(w_{c} \mid w_{r}\right)}
$$

$w_c$ 是context word，$w_r$ 是response word. 为了选择对回复来说最重要的词，作者也计算了$\operatorname{PMI}\left(w_{c}, w_{q}\right)$, $w_q$是last utterance的词, 最终的PMI分数为

$$
\operatorname{PMI}\left(w_{c}, q\right)=\sum_{w_{q} \in q} \operatorname{PMI}\left(w_{c}, w_{q}\right) \\ 
\operatorname{PMI}\left(w_{c}, q, r\right) = \operatorname{norm}\left(\operatorname{PMI}\left(w_{c}, q\right)\right)+\operatorname{norm}\left(\operatorname{PMI}\left(w_{c}, r\right)\right)
$$

然后选择PMI分数最高的20%词插入last utterance q. 

**Pseudo Data Generation:** 在生成阶段, 每一个关键字前后2个位置的临近词都被考虑作为插入词. 在如何选择插入位置这个问题上, 语言模型多层RNN被选择作为解决方案, 同时保留得分最高的三个生成句. 这三个生成句被输送到一个encoder-decoder的生成模型$M_{s2s}$以及一个回复选择模型$M_{ir}$当中:

$$
L_{M_{s 2 s}}\left(r \mid s^{*}\right)=-\frac{1}{n} \sum_{i=1} \log p\left(r_{1}, \ldots, r_{n} \mid s^{*}\right)\\
L_{M_{i r}}\left(p o, n e, s^{*}\right)=M_{i r}\left(p o, s^{*}\right)-M_{i r}\left(n e, s^{*}\right)
$$

$r$是候选回复, $q_r$是CRN生成的候选query, 以及pseudo rewritten candidate $q^*$

**Fine-Tuning with Reinforcement Learning:**

通过上述方式得到的pseudo data不可避免的包含一定程度的错误和干扰, 为了使得回复更加流畅, 引入强化学习来加强CRN模型. 对于生成后的改写语句候选句$q_r$, 计算

回复生成的奖励: 

$$
R_{g}\left(r, q^{*}, q_{r}\right)=L_{M_{s 2 s}}\left(r \mid q^{*}\right)-L_{M_{s 2 s}}\left(r \mid q_{r}\right)
$$

回复选择的奖励:   

$$
R_{i r}\left(p o, n e, q^{*}, q_{r}\right) =L_{M_{i r}}\left(p o, n e, q_{r}\right) - L_{M_{i r}}\left(po, ne, q^{*}\right)
$$

配合经典的policy gradient算法, 计算RL objective的损失函数

$$
\nabla_{\theta} J(\theta)=E\left[R \cdot \nabla \log \left(P\left(y_{t} \mid x\right)\right)\right]
$$

最后把MLE loss 和RL loss的和作为最后的训练目标:

$$
L_{c o m}=L_{r l}^{*}+\lambda L_{M L E}
$$

## [ACL 2020] CorefQA: Coreference Resolution as Query-based Span Prediction (Wu  et  al.,  2020)

香浓科技官方知乎文章:[https://zhuanlan.zhihu.com/p/126544790](https://zhuanlan.zhihu.com/p/126544790)

**Overall:**

把指代消解问题转换为一个query-based Span预测问题, like Extractive QA Task — Query: 一个基于相应代词周围上下文生成的问题, Answer: 一个Clusters包含所有相同指代含义的Coreferences.

1. 这个span prediction策略在mention proposal stage具有很强的灵活性，可以检索出遗漏的mention; 避免mention遗漏造成错误传递
2. 用query明确地去encoding the mention和它的context，这样通过MRC训练框架以及attention机制能够将上下文的影响传播到每一个输入词中, 从而深入透彻地理解coreferent mention的context
3. 大量现有的MRC数据集(SQuAD etc)可用于数据增强，以提高模型的泛化能力

**模型构造:**

该模型由一个**指称提取模块(Mention Proposal)**和一个**指称链接模块(Mention Linking)**组成，

**Mention Proposal:** 使用FFNN从原始文本中提取出所有可能的指代词 Mention, 获得$e_i$的指称得分:

$$
s_{\mathrm{m}}(i)=\mathrm{FFNN}_{\mathrm{m}}\left(\left[x_{\mathrm{FIRST}(i)}, x_{\mathrm{LAST}(i)}\right]\right)
$$

在得到所有文段的得分之后，只取前$\lambda N$个作为候选指称, N为文档长度, $\lambda$ 为超参数.

**Mention Linking:** 将指称聚类为共指词 — 对候选指称中的任意一个指称$e_i$, 指称链接模块任意一个其他候选指称 $e_j$，计算它们是共指的得分. 把$X$作为上下文, $e_i$所在的句子作为query $q(e_i)$,  $e_i$的所有coreferent mentions作为Answers $a$, 得到三元组 \{context, query, answer\}. 考虑到一个mention会有多个匹配的coreferent mentions, **SpanBERT**被用来获取contextual representations 从而给$X$中的每个$x_i$生成BIO tagging:

$$
p_{i}^{t a g}=\operatorname{softmax}\left(\mathrm{FFNN}_{\operatorname{tag}}\left(\boldsymbol{x}_{i}\right)\right)
$$

把token score拓展到span score可以得到$e_j$对$e_i$的共指得分

$$
s_{a}(j \mid i)=\frac{1}{\left|e_{j}\right|}\left[\log p_{\text {FIRST }(j)}^{B}+\sum_{k=\operatorname{FIRST}(j)+1}^{k=\mathrm{LAST}(j)} \log p_{\mathrm{k}}^{I}\right]
$$

根据其对称的双向关系, 可以得到$e_i$和$e_j$的共指得分$s_{a}(i, j)$

$$
s_{a}(i, j)=\frac{1}{2}\left(s_{a}(j \mid i)+s_{a}(i \mid j)\right)
$$

最终$e_i$和$e_j$的共指词得分可以由以下等式获得

$$
s(i, j)=s_{m}(i)+s_{m}(j)+s_{a}(i, j)
$$

即, 他们是共指的前提是 1)他们分别是mention 2)他们互相是coreferent mentions

## [NAACL 2019] Scaling Multi-Domain Dialogue State Tracking via Query Reformulation (Rastogi et  al.,  2019)

来自AI Amazon Alexa AI的工作, 主要是想解决多轮对话下的对话状态跟踪(dialogue state tracking)以及指代消解问题(referring resolution tasks). 同样地, 把任务重构为解决基于**对话内容自感知的语句重写任务(dialogue context-aware user query reformulation task)**, 下图是一个Amazon Alexa的实际例子, CQR Engine可以将用户的输入"Buy his latest book" 转化为"But Yuval Harari's latest book." . 在文中, pointer-generator network(PGN,  See  et  al.,  2017)同样被使用到, 并且使用了multi-task learning来训练这个PGN, 可以做到在训练中**不需要额外的人工标注数据**.

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210203195629925.png" alt="image-20210203195629925" style="zoom:80%;" />

计算PGN中中的generation probability $p_{gen}\left(y_{t} \mid \cdot\right)$ 和copy probability  $p_{copy}\left(y_{t} \mid \cdot\right)$以及一个模式选择概率$p^{mix} \in (0,1)$, 最后得到输出分布:

$$
p\left(y_{k}\right)=p^{\operatorname{mix}} p^{\mathrm{gen}}\left(y_{k}\right)+\left(1-p^{\operatorname{mix}}\right) p^{\operatorname{copy}}\left(y_{k}\right)
$$

在训练数据中, 维护了一个rewrites-corpus $\{x_{it}, y*_{itj}\}^{I,T,J}_{i=1,t=1,j=1}$ 其中i,t,j分别表示：第i个对话数据，某个对话中的第t轮，j表示正确改写中的第j个改写(多个golden reference). 训练目标则是优化如下log-likelihood:

$$
\arg \max _{\theta} \sum_{i, t, j, k} \log p_{\theta}\left(y_{i, t, j, k}^{*}\right)
$$

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210203203401074.png" alt="image-20210203203401074" style="zoom: 67%;" />

上面这个例子中我们可以看到, $y*_{t=2}$包含两个golden reference. 尽管他们语序以及所包含的字词都不相同, 但是Entity S1和U3都出现其中. 这意味着，对于对话重写任务，从输入对话框复制的实体子集应该保持不变，而不管decoder state的动态. 也就是说, 对于task-oriented型语句重写任务语句改写场景下, **关键实体信息**不应该有缺失. 为此, 加入SLU(Spoken Language Understanding)模块识别出来的domain和intent信息, e.g. BOOKQUERY和SYSTEM INFORMINTENT.  此外还需要一步**去词化(delexicalize)**操作: 即用该实体类型或名称替换掉具体的实体value e.g. "Sapiens" -> "book".

![image-20210203205617506](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210203205617506.png)

Multi Task Learning (MTL)中, 可以引入一个额外的**Entity-Copy Auxiliary Objective**用来判断Input中的Entity是否在golden rewrites中. 理论上这个额外的训练目标会优化encoder产生的表征信息, 使其更关注于是否copy这个entity. 最后的PGN的目标函数如下:

$$
\sum_{i, t, j, k} \log p\left(y_{i, t, j, k}^{*}\right)+\lambda \sum_{i, t} \sum_{l=1}^{\left|x_{i, t}\right|} e_{i, t, l} \log g_{\phi}\left(h_{i, t, l}\right)
$$

其中$\lambda > 0$是一个超参数, $e_{i, t, l}$为

$$
e_{i, t, l}=
\left\{
\begin{aligned}
1 & \text { if } x_{i, t, l} \text { is an entity and } x_{i, t, l} \in \mathcal{Y}_{i, t}^{*} \\
-1 & \text { if } x_{i, t, l} \text { is an entity and } x_{i, t, l} \notin \mathcal{Y}_{i, t}^{*} \\
0 & \text { Otherwise }
\end{aligned}
\right.
$$

## [EMNLP 2019] GECOR: An End-to-End Generative Ellipsis and Co-reference Resolution Model for Task-Oriented Dialogue (Quan et  al.,  2020)

Code: https://github.com/terryqj0107/GECOR

Oral: https://vimeo.com/424412465

Web: https://multinlp.github.io/GECOR/

提出了**G**enerative Ellipsis and **CO**-reference **R**esolution model (GECOR), 主要思想是在生成模式和复制模型之间切换来生成一个语义完善的话语.  又搞了一个数据集[CamRest676](https://multinlp.github.io/GECOR/), CamRest676数据集包含676个对话，有2744个用户话语. 这2744个用户话语中有1,174 ellipsis 和1,209 Co-reference 被标注出来. 参考Su  et  al.,  2019的数据分割方案, CamRest676包含1,331 个包含ellipsis或co-reference的正例, 以及不需要改写的负例1413个.

![image-20210203143933026](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210203143933026.png)

GECOR不考虑ellipsis或co-reference的语法特性, 它们可以是字词, 短语甚至短句. 同时这种改写方式不需要提供一组要解析的候选指代, 以往的研究, such as Lee et  al.,  2018往往需要在存在多个ellipsis或co-reference的情况下遍历文本，计算复杂度较高。

embedding layer采用Glove, Utterance and Context Encoder负责分别对待改写句子和对话内容进行编码. 在Decoder中, 需要分别计算与Gu et al., 2016类似的generation probability $p_{g}\left(y_{t} \mid \cdot\right)$ 和copy probability  $p_{c}\left(y_{t} \mid \cdot\right)$:

**generation probability:** 
$$
\begin{array}{l}
P^{g}\left(y_{t}\right)=\frac{1}{Z} e^{\psi_{g}\left(y_{t}\right)}, \quad y_{t} \in \mathbf{V} \\
\psi_{g}\left(y_{t}=v_{i}\right)=\mathbf{v}_{i}^{T}\left(W_{g}^{h} h_{t}^{*}+W_{g}^{s} s_{t}+b_{g}\right) \\
s_{t}=\operatorname{GRU}\left(\left[y_{t-1} ; h_{t}^{*}\right], s_{t-1}\right)
\end{array}
$$

其中$\mathbf{V}$为entire vocabulary, $s_{t-1}$为前一步decoder state, $y_{t-1}$是前一步生成的word embedding, $Z$是归一项.

**copy probability**

对于dialogue context中的每一个词, 是否将其拷贝的概率$P^{c}(y_{t})$如下:

$$
\begin{array}{l}
P^{c}\left(y_{t}\right)=\frac{1}{Z} \sum_{i: c_{i}=y_{t}}^{|\mathbf{C}|} e^{\psi_{c}\left(c_{i}\right)}, \quad y_{t} \in \mathbf{C} \\
\psi_{c}\left(y_{t}=c_{i}\right)=\sigma\left(W_{c} h_{i}^{c}+b_{c}\right) s_{t}
\end{array}
$$

$h_{i}^{c}$是context encoder对于$c_i$的输出结果. 

充分考虑上述两个概率, 可以得到在extended vocabulary = $\mathbf{V} \cup \mathbf{C}$上的选词概率分布公式为:

$$
P\left(y_{l}\right)=P^{g}\left(y_{\iota}\right)+P^{c}\left(y_{l}\right), y_{\iota} \in \mathbf{V} \cup \mathbf{C}
$$

上述copy机制(Gu et al., 2016)可以替换为基于See  et  al., (2017)修改而来的gated copy mechanism, 即引入一个$p_{g e n} \in (0,1)$:

$$
\begin{array}{l}
p_{g e n}=\sigma\left(W_{h} h_{L}^{*}+W_{s} s_{\ell}+W_{y} y_{l-1}+b_{l}\right) \\
P\left(y_{l}\right)=p_{g e n} P^{g}\left(y_{l}\right)+\left(1-p_{g e n}\right) P^{c}\left(y_{l}\right)
\end{array}
$$

将GECOR集成到一个end-to-end的task-oriented对话系统TSCP ([Lei et al. 2018](https://www.aclweb.org/anthology/P18-1133/))当中. 可以从下图中看到, 这个end-to-end架构, 一共包含2个encoder (分别对user utterance和上下文进行编码), 3个decoders: 一个decoder用于预测对话状态，第二个decoder用于生成完整的用户话语，第三个decoder用于生成系统响应.

![image-20210203143958584](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210203143958584.png)

目前只关注GECOR的结果, 暂时不看TSCP的:

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210203171548373.png" alt="image-20210203171259002" style="zoom:80%;" />

baseline model 来自 [Zheng et al. (2018)](http://jcip.cipsc.org.cn/EN/abstract/abstract2686.shtml) 的seq2seq neural network model.

## [AAAI 2020] Filling Conversation Ellipsis for Better Social Dialog Understanding (Zhang  et  al.,  2020)

官方博客: https://xiyuanzh.github.io/projects/AAAI2020.html

Code and Data: https://gitlab.com/ucdavisnlp/filling-conversation-ellipsis

省略(ellipsis)现象在日常对话当中十分常见, ellipsis的存在也增加了**对话角色预测(Dialog Act Prediction), 语义角色标注(Semantic Role Labeling)**等下游任务的难度. 通常会采用自动补全的方法来填充ellipsis, 但是自动补全的话语可能会重复或错过一些单词, 甚至可能产生无意义的句子. 本文提出了一种混合模型**Hybrid-ELlipsis-CoMPlete (Hybrid-EL-CMP)**, 充分考量带有original utterance with ellipsis以及自动补全的utterance, 以提高语言理解能力. 

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210203140601052.png" alt="image-20210203140601052" style="zoom:80%;" />

让我们来看一下上表中的第二个例子. 用户在说完"Okay"之后停顿了一下说"Let’s change conversation", 实际上他的真实意图是想结束对话. 而自动补全的User response会让系统误以为用户表示同意. 本文提出的Hybrid-EL-CMP正是要解决automatically completed utterance所产生的错误, 以便于在下游的Dialog Act Prediction不误导用户意图, 以及Semantic Role Labeling中能正确标识语句成份. 上述两个下游任务, 也是本文的evaluation tasks.  Hybrid-EL-CMP作为一种辅助校验模型, 可以为下游任务提供更好的语言表达能力, 但我们目前更关心如何自动补全不完整的话语.

## [ACL 2020] Generate, Delete and Rewrite: A Three-Stage Framework for Improving Persona Consistency of Dialogue Generation (Song et al., 2020)

腾讯AI又来了, 真的高产.

## [EMNLP 2020] Semantic role labeling guided multi-turn dialogue rewriter (Xu et al., 2020)

腾讯AI三杀了.  这篇文章指出, 上述前序工作的decoder主要使用global attention从而关注对话语境中的所有单词. 由于没有预先引入先验焦点(prior focus), 上述atention机制的注意力可能会对一些无关紧要的字词吸引. 很自然地可以想到— 使用**Semantic Role Labeling (SRL)**识别句子的**谓词-实参(Predicate-Argument)**结构，从而捕获 ***who did what to whom***这样的语义信息作为先验辅助decoder. 下图这个例子很好地体现了语义成分在Utterenace中起到的作用, “粤语”和“普通话”被识别为两个不同的实参, 可以在SRL的指导下获得更多的关注. 

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210204014253359.png" alt="image-20210204014253359" style="zoom:80%;" />

腾讯AI的标注团队从Duconv[(Wu et al., 2019)](https://arxiv.org/abs/1906.05572)数据集上, 标注了3000个 dialogue sessions, 包含33,673个谓词, 27,198个话语. 选用了一个**额外的**BERT-based SRL model [(Shi and Lin, 2019)](https://arxiv.org/abs/1904.05255)作为SRL parser用来完成这个谓词-实参结构的识别任务, 并且在CoNLL 2012 (117,089 examples)进行初步预训练. 

![image-20210204142413045](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210204142413045.png)

Input Representation还是BERT经典三件套Toekn Embedding, Segment Type Embeddings和Position Embeddings, $E_A, E_B$用以区分Speakers. Input由PA structures, dialog context, and rewritten utterance三者拼接而成, 其中PA structures由本质上的以谓词为根, 语义实参为叶子的根转为换<predicate, role, argument>的线性三元组, 并以随机顺序拼接.  这样以随机顺序i拼接的机制可能会对sequence encoder带来一定的干扰信息. 这边引入附加在PA sequence上的bidirectional attention mask机制来辅助, 即不同PA三元组中的token不能相互attend. 同时PA三元组的position embedding使用各个三元组的独立位置信息.

在REWRITE(Su  et  al.,  2019)数据集上进行实验对比一下结果, BERT指代RoBERTa Chinese. **复现结果与Su  et  al.,  (2019)不太一致.**

![image-20210204145330846](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210204145330846.png)

**Ablation Study:**

* SRL: 来自经由CoNLL 2012训练的SRL parser, 75.66 precision, 74.47 recall, and 75.06 F1
  * Bi-mask: PA Triples中的token可以互相关注
  * Triple-mask: 只允许在同一个Triple的Token互相关注, 不同的Triples中的token彼此是不可见的 -> 独立地编码每个谓词-参数三元组，这可以防止不必要的三元内部注意，从而更好地模拟SRL结构

* Partial-SRL: SRL Model只作用于待改写句

* Gold-SRL: **使用SRL的golden label作为输入**, 说明SRL model的质量高度影响rewriter.

在Restoration-200K(Pan et al., 2019)数据集上实验结果如下, 还不如Pan et al., (2019)提出的PAC Model.

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210204150311732.png" alt="image-20210204150311732" style="zoom:80%;" />

> we find that the SRL information mainly improves the performance on the dialogues that require information completion. One omitted information is considered as properly completed if the rewritten utterance recovers the omitted words. We find the SRL parser naturally offers important guidance into the selection of omitted words

## [EACL 2021] Ellipsis Resolution as Question Answering: An Evaluation (Aralikatte et  al.,  2021)

文章把省略补充问题和指代消解问题转化为QA问题, 如下query可以转化为<context, question, answer>的三元组: 

* context: the entire document

* question: the sentence in which the ellipsis/mention is present

* answer:  the antecedent/entity 

对于指代消解问题, 如果一个句子包含n个mentions, 就转为成为n个questions其中resolution用<ref></ref>包裹.

P.S. Ellipsis被分为Sluice Ellipsis和Verb Phrase Ellipsis, 分别有不同的公开数据集.

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210204000806314.png" alt="image-20210204000806314" style="zoom:80%;" />

**QA Architectures**

选取三种不同的encoder module:

1. DrQA (Chen et al., 2017), LSTM
2. QANet (Yu et al., 2018), CNN
3. BERT (Devlin et al., 2018), pre-trained Transformer 

简单看下结果:

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210204003514239.png" alt="image-20210204003514239" style="zoom:80%;" />

很奇怪这篇文章没有跟coreference task的一些工作进行同向比较, 他们这样解释:

> In this section, we analyse the best performing coreference models and discuss why they cannot be compared with other works in literature.

## [WSDM 2021] Question Rewriting for Conversational Question Answering (Vakulenko et  al.,  2021)

本文展示了如何使用QR组件扩展现有的SOTA QA模型，并演示了所提出的方法提高了QA任务在End-to-end会话下的性能。通过QR组件重写话语, 减少了follow-up questions的模糊性的，使它们可以被现有的QA模型作为对话上下文之外的独立问题来处理。

提出了基于Transformer++作为Question Rewriting Model, 并将其嵌入于两种不同的QA任务类型中, 关注会话场景下QR任务中在不同类型QA模型中的应用能力.

1. retrieval QA: 即在给定的文章中, 根据已有<问题, 答案>匹配关系集合寻找答案
2. extractive QA: 即在一篇文章中为一个给定的自然语言问题找到答案。

主要贡献在于在TREC CAsT 2019 passage retrieval dataset以及CANARD数据集上进行相关实验, 对比了相关baseline:

![image-20210302132514070](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210302132514070.png)

## [ArXiv 2020] Robust Dialogue Utterance Rewriting as Sequence Tagging (Hao et  al.,  2020)

Paper Link: https://arxiv.org/abs/2012.14535

这篇来自Xu et al., (2020)同一个团队的文章指出, 从对话内容中拷贝缺失内容来重写语句的方式受限于不同Domain. 文本生成需要往往需要对每个字计算output probability, 这就使得搜索空间随着字典大小和上下文对话长度的增加而扩大. 此外, 暴露偏差(exposure bias, Wiseman和Rush, 2016)会进一步加剧测试用例与训练集不相似的问题，导致输出与输入表达不同的语义含义. 文本提出了一种新颖的sequence-tagging-based model来解决这一健壮性问题, 使得搜索空间大大减少. 同时为了解决缺乏流畅性这一文本生成的标注模型的通病, 作者在一个REINFORCE framework下注入来自BLEU或GPT-2的损失信号. 

**R~A~ST: Rewriting as Sequence Tagging:**

先前的工作往往对于ellipsis和co-reference采取不同的策略, 往往是Insert, Repalce. R~A~ST将Insert和Repalce这两个操作用一个先deletion后Insetion的操作代替. 标注数据由longest common sub-sequence (LCS)算法生成, 剔除REWRITE(Su  et  al.,  2019)和RESTORATION-200K(Pan et al., 2019中不满足要求的.

![image-20210204161459033](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210204161459033.png)

* Deletion  $\in \{0, 1\}$: the word $x_n$ is deleted (i.e. 1) or not (i.e. 0);
* Insertion: [start, end]: 对话内容中的位置信息, If no phrase is inserted, the span is [-1, -1]. 

Insertion中的span prediction当作MRC处理.

**Enhancing Fluency with Additional Supervision:**

REINFORCE的强化学习目标:

$$
\mathcal{L}_{r l}=\left(r\left(\hat{u}_{i}^{g}, u_{i}\right)-r\left(\hat{u}_{i}^{s}, u_{i}\right)\right) \log p\left(\hat{u}_{i}^{s} \mid X\right)
$$

其中$\hat{u}_{i}^{g}$和$\hat{u}_{i}^{s}$分别来自生成和采样的candidate sentences, $r(\cdot, \cdot)$是reward function 来自sentence-level BLEU或者GPT-2 model, 可以得到最后的损失函数:

$$
\mathcal{L}=(1-\lambda) \mathcal{L}_{\text {tagging }}+\lambda \mathcal{L}_{r l}
$$

**结果分析:**

![image-20210204164236489](https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210204164236489.png)

## [ArXiv 2020] MLR: A Two-stage Conversational Query Rewriting Model with Multi-task Learning (Song et  al.,  2020)

Paper Link: https://arxiv.org/abs/2004.05812

在该模型中, 语句重写被定义为一个序列生成问题，并通过辅助的词类标签(word category label)预测任务引入词类信息.

<img src="https://zheyuye-image-1257819557.cos.ap-shanghai.myqcloud.com/img/image-20210204165134805.png" alt="image-20210204165134805" style="zoom:80%;" />

$$
L=L g+L c=-\sum_{i} \log p\left(y_{i}\right)-\sum_{j} \log p\left(c_{j}\right)
$$

其中$p(y_{i})$是目标生成词的预测概率, $p(c_{j})$是词类标签的预测概率.