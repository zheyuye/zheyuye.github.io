---
title: 'Reivew Adam with me'
date: 2021-09-28
permalink: /review_adam_with_me
tags:	
  - optimization
---

# 2021年来跟我一起复习Adam

[Adam](https://arxiv.org/abs/1412.6980)作为一种自适应的优化算法, 结合了Momentum以及RMSprop算法, 一方面参考动量作为参数更新方向, 一方面计算梯度的指数加权平方:
$$
M_t = \beta_1 M_{t-1} + (1-\beta_1) \boldsymbol{g}_{t} \\
G_t = \beta_2 G_{t-1} + (1-\beta_1) \boldsymbol{g}_{t} \odot \boldsymbol{g}_{t}
$$

通常我们会需要对$M_t$以及$G_t$进行偏差修正(bias correction):
$$
\hat{M_t} = \frac{M_t}{1-\beta^t_1} \\
\hat{G_t} = \frac{G_t}{1-\beta^t_2}
$$

从而计算参数更新差值为:

$$
\begin{align*}
\Delta\boldsymbol{\theta}_{t} &= - \frac{\alpha} {\sqrt{\hat{G_t} + \epsilon}} \hat{M_t} \\ &= - \alpha \frac{M_t}{1-\beta^t_1} \frac{1-\beta^t_2}{G_t} \\
&= - \alpha \frac{\sqrt{{1-\beta^t_2}}}{{1-\beta^t_1}}  \frac{M_t}{\sqrt{G_t + \epsilon}}
\end{align*}
$$

其中$\alpha$是学习率, 也就是我们常说的learning rate. 但是在实际implement中, 在代码实现层面通常会将bias correction加入到$\alpha$的计算中, 即:

$$
\hat{\alpha} = \alpha \frac{\sqrt{{1-\beta^t_2}}}{{1-\beta^t_1}}
$$
实际上得到的参数更新差值为:

$$
\Delta\hat{\boldsymbol{\theta}_{t}} = - \frac{\hat{\alpha}}{\sqrt{G_t + \epsilon}} M_t
$$


代码层面, 带bias correction的adam算法可以简单的表示为[(implemented by mxnet](https://github.com/apache/incubator-mxnet/blob/5722f8b38af58c5a296e46ca695bfaf7cff85040/python/mxnet/optimizer/adam.py))

```python
rescaled_grad = clip(grad * rescale_grad, clip_gradient) + wd * weight
m = beta1 * m + (1 - beta1) * rescaled_grad
v = beta2 * v + (1 - beta2) * (rescaled_grad**2)
lr = learning_rate * sqrt(1 - beta2**t) / (1 - beta1**t)
w = w - lr * m / (sqrt(v) + epsilon)
```

然而[BERT等相关预训练模型](https://github.com/google-research/bert/blob/master/optimization.py#L87)当中实际上并没有应用bias correction这个策略, 不带bias correction的adam算法表示为:

```python
grad = clip(grad * rescale_grad, clip_gradient) + wd * weight
m = beta1 * m + (1 - beta1) * grad
v = beta2 * v + (1 - beta2) * (grad**2)
lr = learning_rate
w = w - lr * (m / (sqrt(v) + epsilon) + wd * w)
```

## Bias Correction

那么为什么我们要进行偏差修正呢, Bias Correction在训练过程中扮演了什么样的作用呢, 我们先来看一下论文原文的解释:

> In case of sparse gradients, for a reliable estimate of the second moment one needs to average over many gradients by chosing a small value of β2; however it is exactly this case of small β2 where a lack of initialisation bias correction would lead to initial steps that are much larger.

举例来说, 在实际训练中通常我们会采用$\beta_1 =0.9$,  $\beta_2 =0.999$作为超参数, 并将$M_0,G_0$初始化$M_0=0, G_0=0$. 将$\beta_1, \beta_2$ 带入上述公式中, 我们不妨来看一下训练初期会发生什么:

$$
\begin{align*}
M_1 &= 0.1 \boldsymbol{g}_{t} \\
G_1 &= 0.001 \boldsymbol{g}_{t}^2 \\
\Delta\boldsymbol{\theta}_{t} &= - \alpha  \frac{M_1}{\sqrt{G_1 + \epsilon}} \\
 &= - \alpha  \frac{0.1 \boldsymbol{g}_{t}}{\sqrt{0.001 \boldsymbol{g}_{t}^2 + \epsilon}}
\end{align*}
$$
可以看到$\Delta\boldsymbol{\theta}_{t}$在模型训练初期会比较大

或者$1-\beta_2$很小的时候, 动量估计值很容易往初始值0的方向偏移.



